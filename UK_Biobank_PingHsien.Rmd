---
title: "UK_Biobank_PingHsien_Yang"
output:
  word_document: default
  html_document: default
date: "2023-11-08"
editor_options:
  markdown:
    wrap: 72
---

```{css, echo=FALSE}
pre {
  font-size: 1em;
}

code {
  font-size: .75em;
}
```
# Introduction

## Cholesterol Impact Analysis on Health Outcome Predictive Models 

This report presents a comprehensive analysis of a dataset that includes a variety of health-related characteristics of a population, with a particular focus on the impact of cholesterol levels on different health outcomes. The analysis involves statistical summaries, correlations, group comparisons by sex and ethnicity, and advanced modeling using a Gradient Boosting Machine (GBM).

The dataset includes various histograms, bar charts, and scatter plots that outline the distribution of age, sex, ethnicity, education level, Townsend deprivation index, BMI, cholesterol levels, physical activity, smoking and drinking status, and the occurrence of dementia, myocardial infarction (MI), and stroke.

## Objectives

The main objectives of the report are:

To elucidate the distribution and influence of cholesterol levels on the population's health outcomes.
To understand how cholesterol interacts with other variables such as age, sex, BMI, and ethnicity.
To evaluate the predictive power of cholesterol levels for outcomes like MI, dementia, and stroke using GBM.

## Variable interpretation

```{r,message = FALSE}
# import necessary datasets
library(dplyr)
library(ggplot2)
library(gridExtra)
library(PerformanceAnalytics)
library(GGally)
library(caret)
library(factoextra)
library(gbm)
library(e1071)
library(modelr)
library(tidyverse)
library(MASS)
library(leaps)
library(nortest)
library(pROC)
library(cowplot)
library(ggplotify)
```

```{r}
summary_group = function(df, groupby, column) {
  result = df %>%
    group_by({{ groupby }}) %>%
    summarise(
      Min. = min({{ column }}, na.rm = TRUE),
      Median = median({{ column }}, na.rm = TRUE),
      Mean = mean({{ column }}, na.rm = TRUE),
      Max. = max({{ column }}, na.rm = TRUE),
      # values = list({{ column }}, na.rm = TRUE)
    )
  return(result)
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo =  FALSE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "100%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Read in dataframe

```{r , message = FALSE}
BM = as.data.frame(read_csv("./Bonus_Midterm_data_setA.csv"))
BM = BM[,-1]
dim(BM)
```

In this Dataset, we have 10000 Rows and 13 Columns (exclude the ID)

------------------------------------------------------------------------

## Statistic summary

```{r , echo=FALSE}
summary(BM)
# str(BM)
```

------------------------------------------------------------------------

## Data cleaning - check missing value

```{r , message = FALSE}
BM_rm=BM[complete.cases(BM),] 
check =nrow(BM) - nrow(BM_rm)
cat("number of missing value : ", check)
```

According to the data, there's no missing values.

## Data cleaning - check error value in categorical variables

```{r , echo=FALSE}

columns = colnames(BM[,c(2:4,9:13)])

for (col_name in columns) {
  unique_values <- unique(BM[[col_name]])
  cat("Column:", col_name, "\n")
  cat("Unique Values:", unique_values, "\n\n")
}

```

According to the data, there's no error values in categorical variables.

## Data cleaning - check outlinet in Numeric variables

```{r, fig.cap="Figure 1 : boxplot for outlier"}
columns = colnames(BM[,-c(2:4,9:13)])
par(mfrow=c(2,3))
for (col_name in columns) {
  unique_values <- unique(BM[[col_name]])
  boxplot(unique_values, main=col_name)
}
```

```{r}
BM = BM[BM$bmi_0 < 59,]
BM = BM[BM$cholesterol_0 < 10,]
BM = BM[BM$MET_activity < 10000,]
```

In the dataset, we detected 3 outliers, which are distributed among
"BMI" variables, shows on the chart.

We further investigated the reasons for the existence of outliers, and
possible reasons include errors in the data collection process or
extreme values.

We Removed outliers to minimize their impact on the results.

## Data cleaning - Remove the Non-Response Data

Ethnicity: remove rows with "Other", "Prefer not to answer", and "Do not
Know" Smoking: remove rows that is "Prefer not to answer" Alcohol:
remove rows that is "Prefer not to answer"

```{r}
BM <- BM[!(BM$ethnicity_group %in% c(6, 997, 999)), ]
BM <- BM[!(BM$smoking_status_0 %in% 9), ]
BM <- BM[!(BM$alcohol_status_0 %in% 9), ]
```

We Removed Non-Response Data to minimize their impact on the results.

------------------------------------------------------------------------

## summary statistics after data cleaning

```{r, fig.cap="Figure 2 : summary statistic"}
BM_dis = BM
BM_dis$sex = as.factor(BM_dis$sex)
plots = list()  # 創建一個空列表來存儲圖像對象

columns = colnames(BM_dis)

for (col_name in columns) {
  
  p = ggplot(BM_dis, aes_string(x=col_name)) + 
    geom_histogram(fill="darkgrey",color="darkslategray", binwidth=1,stat="count") +  
    labs(title=col_name, y="Count")
  
  plots[[length(plots) + 1]] = p  # add p into list
  
}

# print
plots$ncol = 4
do.call(grid.arrange, plots) 

```

In this dataset,

**age:** This is a histogram showing the distribution of ages. It looks
like a more normal distribution, but may be right-skewed (most
individuals are clustered on the lower age side).

**sex:** This is a bar chart showing the distribution of sex in the
dataset. There are two categories: female (F) and male (M). As you can
see, the number of females in the sample is larger than the number of
males.

**ethnicity_group:** This bar chart shows the distribution of different
ethnic groups. Ethnicity 1 (white) is in the majority.

**education_college_university:** This histogram appears to show the
distribution of college-educated people with a larger number of
non-college professors.

**townsend_deprivation_index:** This is a histogram showing the
distribution of the Townsend Deprivation Index. This index may be a
measure of the socio-economic status of the area. It is left-skewed
(most individuals are concentrated on the lower side of the index).

**bmi_0:** This histogram shows the distribution of body mass index
(BMI). The data appears to be more normally distributed and concentrated
within a certain interval.

**cholesterol_0:** This is a plot of the distribution of blood
cholesterol levels. The data appears to be close to normally
distributed, perhaps slightly skewed to the right.

**MET_activity:** This histogram represents a measure of exercise. The
distribution shows a lot of small peaks, which may represent a breakdown
of different activity levels.

**smoking_status_0:** This bar graph shows the distribution of smoking
status, coded with numbers to indicate different smoking statuses.

**alcohol_status_0:** This is another bar chart showing the distribution
of drinking status, again coded with numbers.

**dementia_all_outcome:** This bar chart shows the distribution of
dementia outcomes, with 0 and 1 indicating the absence and presence of
dementia.

**MI_all_outcome:** This bar graph shows the distribution of all
myocardial infarction outcomes, with 0 and 1 indicating the absence and
presence of myocardial infarction.

**stroke_all_outcome:** This bar chart shows the distribution of all
stroke outcomes.

### Marginal Correlation and Correlation martix

```{r, include=FALSE, warning=FALSE}
corr_matrix = 
  BM %>% 
  dplyr::select(age, townsend_deprivation_index, bmi_0, cholesterol_0, MET_activity, education_college_university_0, smoking_status_0, alcohol_status_0, dementia_all_outcome, MI_all_outcome, stroke_all_outcome) %>% 
  chart.Correlation(histogram = TRUE, method = "spearman")


```

### Correlation Heatmap

```{r, fig.cap="Figure 3 : Correlation Heatmap"}

BM %>%
  dplyr::select(age, 
                townsend_deprivation_index, 
                bmi_0, 
                cholesterol_0, 
                MET_activity, 
                education_college_university_0, 
                smoking_status_0, 
                alcohol_status_0, 
                dementia_all_outcome, 
                MI_all_outcome, 
                stroke_all_outcome) %>%
  ggcorr(method = c("everything", "spearman"),
         label=TRUE,
         hjust = 0.9,
         layout.exp = 0.5,
         label_size = 3,
         label_round = 2,
         label_alpha = 0.8,
         colors = c("darkslategray3","white","coral")) +

  ggtitle("Correlation Heatmap") +
  theme(plot.title = element_text(hjust = 0.5))
  #標題居中

```

The heatmap shows that most variables have a very low correlation with
each other since the majority of the values are close to 0. However,
there are some notable correlations, such as:

1.A moderate **positive** correlation (0.13) between
**dementia_all_outcome** and **stroke_all_outcome**, suggesting that
individuals who have had a stroke might be more likely to be diagnosed
with dementia.

2.A small **positive** correlation (0.11, 0.12) between **age**,
**dementia_all_outcome**, **MI_all_outcome** and **stroke_all_outcome**,
implying that the likelihood of having a heart attack or stroke may
increase with age.

3.A small **negative** correlation (-0.1) between **cholesterol** and
**MI_all_outcome**,This means that with a slight decrease in cholesterol
levels, there was a slight increase in the incidence of all outcomes of
heart attacks.

4.A small **positive** correlation (0.09) between **BMI** and
**MI_all_outcome**, implying that the likelihood of having a heart attack
with higher BMI.

5.A small **positive** correlation (0.08) between
**townsend_deprivation_index** and both **BMI** and **Smoke_status**,
suggesting that as deprivation increases, there tends to be a slight
increase in BMI and a higher likelihood of smoking. But a
small**negative** correlation (-0.08) with **alcohol_status**, where higher
deprivation scores are associated with a slight decrease in
alcohol_status.

However, those associations are very weak, suggesting that there is no
strong direct relationship between each variables. Those slight
correlation between each are not sufficient to indicate a clear trend or
to inform clinical decisions. More comprehensive studies are needed to
further investigate the relationship between these variables.

```{r, fig.cap="Figure 4 : gendar choleserol comparison"}

s_c = summary_group(BM,sex,cholesterol_0)
s_c = as.data.frame(s_c)

plots_sex = list()

for (col in colnames(s_c[,-1])) {
  
  p = ggplot(s_c, aes_string(x = "sex", y = col, fill = col)) + 
        geom_col() +  
        labs(title = col, y = "cholesterol")+
        scale_fill_gradient(low = "darkslategray3", high = "darkslategray")  

  
  plots_sex[[length(plots_sex) + 1]] = p  # add p into list
  
}

plots_sex$ncol = 4
do.call(grid.arrange, plots_sex) 

```

This image presents a set of four bar charts, each one representing a
different statistical measure (Minimum, Median, Mean, and Maximum) of
cholesterol levels across Male and Female. The cholesterol levels are on
the vertical axes, and the Sex are on the horizontal axes.

As can be seen from the bar graphs, females have slightly higher
cholesterol than males. However, Males and Females in this particular
dataset are very close to each other in terms of minimum, median, mean,
and maximum cholesterol values. This may mean that gender has little
effect on cholesterol levels, but more analysis is needed to support
this conclusion, considering standard deviation and other distributional
characteristics.

```{r, fig.cap="Figure 5 : ethnicity cholesterol comparison"}

e_c = summary_group(BM, ethnicity_group, cholesterol_0)
e_c = as.data.frame(e_c)
e_c <- e_c %>% 
  mutate(ethnicity_group = recode(ethnicity_group,
                                  `1` = "White",
                                  `2` = "Mixed",
                                  `3` = "Asian",
                                  `4` = "Black",
                                  `5` = "Chinese"))

plots_e = list()
for (col in colnames(e_c[,-1])) {
  
  p = ggplot(e_c, aes_string(x = "ethnicity_group", y = col, fill = col)) + 
        geom_col() +  
        labs(title = col, y = "cholesterol")+
        scale_fill_gradient(low = "darkslategray3", high = "darkslategray")
  
  plots_e[[length(plots_e) + 1]] = p  # add p into list
  
}

plots_e$ncol = 2
do.call(grid.arrange, plots_e) 

```

This image presents a set of four bar charts, each one representing a
different statistical measure (Minimum, Median, Mean, and Maximum) of
cholesterol levels across five different ethnicity groups (Asian, Black,
Chinese, Mixed, and White). The cholesterol levels are on the vertical
axes, and the ethnicity groups are on the horizontal axes.

As can be seen from the bar graphs,The lowest minimum value of
cholesterol is seen in the Mixed ethnicity group. The highest maximum
values of cholesterol are seen in the Asian and White ethnicity groups,
both just below 9.5, while the lowest maximum is in the Chinese
ethnicity group, around 8.0.

The uniformity of median and mean levels suggests that there's not a
significant difference in central cholesterol levels among these ethnic
groups, but there may be differences in the range (minimum and maximum),
which could be important for medical or health-related studies.

### Checking whether cholesterol_0 is normal distributed ot not

```{r, message = FALSE}

BM_ml = BM

ks.test(BM_ml$cholesterol_0, "pnorm", mean=mean(BM_ml$cholesterol_0), sd=sd(BM_ml$cholesterol_0))
```

The output provided is from an Asymptotic one-sample Kolmogorov-Smirnov
(K-S) test. This test is a nonparametric test used to determine whether
a sample comes from a specified distribution.

p-value = 0.003266: The p-value indicates the probability of observing a
test statistic as extreme as, or more extreme than, the one observed if
the null hypothesis is true. In this case, the null hypothesis is that
the sample distribution is the same as the reference distribution. The
p-value is 0.003266, which is less than 0.05, suggesting that there is a
statistically significant difference between the distribution of
cholesterol_0 and the reference distribution, which means the
*cholesterol_0 variable is not normal distributed*.

### split dataset

```{r, message = FALSE}

BM_ml <- BM_ml %>%
  dplyr::select(-dementia_all_outcome, 
                -MI_all_outcome, 
                -stroke_all_outcome)%>%
  mutate(sex = recode(sex,
                      `M` = 1,
                      `F` = 0))
splitIndex <- createDataPartition(BM_ml[,1], p = 0.7, list = FALSE)

BM_training <- BM_ml[splitIndex, ]
BM_test <- BM_ml[-splitIndex, ]
```

### using Box--Cox transformation

```{r, message = FALSE}

# 执行Box-Cox变换的似然比检验来找到最佳的lambda
bc <- boxcox(cholesterol_0 ~ ., data = BM_training)

# 找到最佳的lambda值
lambda_best <- bc$x[which.max(bc$y)]

transformed_cholesterol_0 <- if(lambda_best == 0) {
  log(BM_training$cholesterol_0)
} else {
  (BM_training$cholesterol_0^lambda_best - 1) / lambda_best
}

# Now you can replace the original cholesterol_0 variable with the transformed one
BM_training$cholesterol_0 <- transformed_cholesterol_0

# 逆变换函数
inverse_boxcox <- function(transformed_data, lambda) {
  if(lambda == 0) {
    return(exp(transformed_data))
  } else {
    return((transformed_data * lambda + 1)^(1/lambda))
  }
}

# check again
ks.test(BM_training$cholesterol_0, "pnorm", mean=mean(BM_training$cholesterol_0), sd=sd(BM_training$cholesterol_0))

```

Use Box-Cox transformations to improve the normality of data.

Find the value of λ that corresponds to the largest y-value (likelihood
value) in the likelihood-ratio test, which is the value of λ that we
believe best transforms the data into a normal distribution. If the best
λ-value is 0, the natural logarithmic transformation is used; if it is
not 0, the transformation is performed using a more complicated formula.

Then use statistical tests after the transformation to test the effect
of the improvement. This will improve the accuracy and predictive power
of the model.

### Compare different model

```{r}
set.seed(1)

cv_df = 
  crossv_kfold(BM_ml[1:2000,], k=10) %>%   # k-fold = 5
  mutate(
    train = map(train, ~ as_tibble(.) %>%
                      mutate_at(vars(-cholesterol_0), scale)),  # Assuming crm_1000 is the target variable, and we don't scale it.
    test = map(test, ~ as_tibble(.) %>%
                    mutate_at(vars(-cholesterol_0), scale))
  )

cv_df = 
  cv_df %>% 
  mutate(
    
    glm_fit = map(.x = train, ~glm(cholesterol_0 ~ ., data = .x)),
    svm_tune_results = map(.x = train, ~tune(svm, cholesterol_0 ~., data = .x, 
                                             ranges = list(cost = 10^(-1:2), gamma = c(0.5, 1, 2)))),
    best_svm_model = map(.x = svm_tune_results, ~.x$best.model),
    svm_fit = map2(.x = best_svm_model, .y = train, ~svm(cholesterol_0 ~., data = .y, type="eps-regression", kernel = "linear", 
                                                         cost = .x$cost, scale = TRUE)),
    gbm_fit = map(.x = train,~gbm(cholesterol_0 ~ ., 
                 data = .x, 
                 distribution = "gaussian",
                 n.trees = 100,
                 shrinkage = 0.01,
                 interaction.depth = 3,
                 cv.folds = 1))
    
  ) %>% 
  mutate(
    rmse_glm = map2_dbl(.x = glm_fit, .y = test, ~rmse(model = .x, data = .y)),
    rmse_svm = map2_dbl(.x = svm_fit, .y = test, ~rmse(model = .x, data = .y)),
    rmse_gbm = map2_dbl(.x = gbm_fit, .y = test, ~rmse(model = .x, data = .y))
  )
```

```{r, fig.cap="Figure 6 : Model RMSE comparison"}
cv_df%>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  mutate(model = fct_relevel(model, "glm","svm","gbm")) %>% 
  ggplot(aes(x = model, y = rmse,))+
  geom_violin(aes(fill = model), alpha = 0.3)+
  geom_boxplot(width = 0.2, fill = "white", color = "gray30", alpha = 0.5) + 
  scale_x_discrete(labels = c("Logistic regression", "Support Vector", "Gradient Boosting"))+
  ggtitle("RMSE Distribution Plots") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none")  +  # the display of legends is redundant
  labs(y = "RMSE", x = "Models")

```

This graph shows the distribution of RMSE (Root Mean Square Error) for
three different statistical models. In forecasting models, RMSE is a
common measure of the difference between the model's predicted value and
the actual observed value. Generally speaking, the lower the RMSE, the
higher the accuracy of the model.

The three models are:

1.**Logistic regression**  2.**Support Vector Models**   3.**Gradient Boosting**

As can be seen from the violin plot, **Gradient Boosting** has lowest
RMSE, which means it has better predictive accuracy. Therefore, for
further analysis and model interpretation, I will choose the *Gradient
Boosting model*, taking into account the stability and high accuracy of
its prediction results.

### cholesterol_0 with tuning

```{r, fig.cap="Figure 7 : cholesterol model interpretatIon"}
gbm_train = BM_training
gbm_test = BM_test

# Define the training control
train_control <- trainControl(method = "cv", number = 5)

# Create the grid of tuning parameters
gbmGrid <- expand.grid(interaction.depth = c(1, 3, 5),
                       n.trees = (1:10) * 100,
                       shrinkage = c(0.01, 0.1),
                       n.minobsinnode = 10)

# Train the model
gbm_tuned_model <- train(cholesterol_0 ~ ., 
                         data = gbm_train, 
                         method = "gbm",
                         trControl = train_control,
                         verbose = FALSE,
                         tuneGrid = gbmGrid,
                         metric = "RMSE",
                         distribution = "gaussian")

# Best model
best_params = gbm_tuned_model$bestTune

gbm_model <- gbm(cholesterol_0 ~ ., 
                        data = gbm_train, 
                        distribution = "gaussian",
                        n.trees = best_params$n.trees,
                        shrinkage = best_params$shrinkage,
                        interaction.depth = best_params$interaction.depth,
                        n.minobsinnode = best_params$n.minobsinnode,
                        cv.folds = 10, # Assuming you want to use the same cv folds
                        verbose = FALSE)

# 预测
predicted <- predict(gbm_model, newdata = gbm_test, n.trees = best_params$n.trees)

# inverse Box-Cox
gbm_test$predict = inverse_boxcox(predicted,lambda_best)

# 計算準確率
MAPE <- mean(abs(gbm_test$predict - BM_test$cholesterol_0) / BM_test$cholesterol_0)
cat(paste("Mean Absolute Percentage Error : ", round(MAPE*100,2), "%", sep = ""))

# 繪製特征重要性
importance <- summary(gbm_model, n.trees = 100, plot = FALSE)
df_importance <- data.frame(feature = rownames(importance), relative_importance = importance$rel.inf)
importance_plot = ggplot(df_importance, aes(x = reorder(feature, -relative_importance), y = relative_importance,  fill = relative_importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Features") + ylab("Relative Importance") +
  scale_fill_gradient(low = "darkslategray3", high = "darkslategray") +
  # ggtitle("Feature Importance")+
  theme(legend.position = "bottom")# 将图例移动到左侧
  


# 繪製特征相關性，前幾个最重要的变量
var_imp <- summary(gbm_model, plot=FALSE)
top_vars <- head(var_imp, 4)$var

# 创建一个空的图形列表
plots_cor <- list()

# 为前6个变量创建偏差图
for (i in seq_along(top_vars)) {
  plots_cor[[i]] <- plot.gbm(gbm_model, i = top_vars[i])
}

# 使用grid.arrange来组合图形
# combined_plots_cor = do.call(gridExtra::grid.arrange, c(plots_cor, ncol=2))
combined_plots_cor <- gridExtra::arrangeGrob(grobs = plots_cor, ncol = 2)

# 使用cowplot将两个图形组合起来
combined_plot <- plot_grid(importance_plot,
                           ggplotify::as.ggplot(combined_plots_cor),
                           labels = c("Feature Importance","partial dependence"),
                           ncol = 2,
                           rel_heights = c(1, 1),
                           rel_widths = c(1,1))
combined_plot
# 
# # 残差
# residuals <- abs(gbm_test$cholesterol_0 - gbm_test$predict)
# cat("\n")
# cat("RMSE : ",mean(residuals))
# gbm_rmes = data.frame(residuals = residuals)
# 
# # 繪製残差圖
# ggplot(gbm_rmes, aes(x = residuals)) + 
#   geom_histogram(binwidth = 0.5, fill = "darkslategray", color = "black", alpha = 0.7) +
#   ggtitle("Residuals Distribution")
# 
# # learning line
# train_error <- gbm.perf(gbm_model, method = "cv")

```

Feature Importance Bar Chart : It shows the relative importance of
individual features to model predictions. Feature importance is a way of
measuring the magnitude of the role that features play in the model's
decision-making process. In this graph, '**age**' (age) is the most
important feature, followed by '**bmi_0**' (body mass index) , '**sex**'
(gender) and '**townsend_deprivation_index.**(deprivation index)' . This
means that these variables have the most influence in the model
predictions.

partial dependence plots : It shows the relationship between the six
most important characteristics and the target variable Y. The graph
shows the relationship between the six most important characteristics
and the target variable Y. In those chart, '**age**': we can see that the
mean of Y starts to decrease after peaking at a certain age point,This
suggests that the relationship between age and Y is important in that
particular age range, which is at **55** to **60**.

'**bmi_0**': This may point to a correlation between high BMI and an
increase in Y(25 to 30), but after a certain point, the relationship
becomes weaker.

'**gender**': Since gender is a categorical variable, what we see is that
different genders may have different average effects on the Y-value . we
can see that female(0) has more impact than male(1) in cholesterol
level.

'**Townsend_deprivation_index**': negative correlate, wiht higher
Townsend_deprivation_index, we can see that cholesterol correlation
became weaker.

Finally, A MAPE of 15.58% means that the model's predicted values will
be in error by 15.58% on average.

### predict the disease by gbm model

```{r}

BM_disease <- BM %>%
  mutate(sex = recode(sex,
                      `M` = 1,
                      `F` = 0))
splitIndex <- createDataPartition(BM_disease[,1], p = 0.7, list = FALSE)

gbm_train <- BM_disease[splitIndex, ]
gbm_test <- BM_disease[-splitIndex, ]
```

### MI

```{r, fig.cap="Figure 8 : MI model interpretatIon"}

gbm_model <- gbm(MI_all_outcome ~ .,
                 data = gbm_train,
                 distribution = "bernoulli", 
                 n.trees = 300,               
                 shrinkage = 0.01,            
                 interaction.depth = 3,     
                 cv.folds = 5)               

predicted <- predict(gbm_model, newdata = gbm_test, type = "response")

# 计算ROC曲线
roc_obj <- roc(gbm_test$MI_all_outcome, predicted)

# 计算Youden's J statistic
youdens_j <- roc_obj$sensitivities + roc_obj$specificities - 1

# 找到最大的Youden's index对应的阈值
optimal_cutoff <- roc_obj$thresholds[which.max(youdens_j)]
cat("Threshold corresponding to the largest Youden's index : ",optimal_cutoff)
# 根据阈值决定类别，通常使用0.5作为阈值
predicted_class <- ifelse(predicted > optimal_cutoff, 1, 0)

# 将预测类别添加到测试数据集中
gbm_test$predict <- predicted_class

# 计算准确率
accuracy <- mean(gbm_test$predict == gbm_test$MI_all_outcome)

cat("\n","Accuracy: ", round(accuracy * 100, 2), "%")

# 繪製特征重要性
importance <- summary(gbm_model, n.trees = 100, plot = FALSE)
df_importance <- data.frame(feature = rownames(importance), relative_importance = importance$rel.inf)
importance_plot = ggplot(df_importance, aes(x = reorder(feature, -relative_importance), y = relative_importance,  fill = relative_importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Features") + ylab("Relative Importance") +
  scale_fill_gradient(low = "darkslategray3", high = "darkslategray") +
  # ggtitle("Feature Importance")+
  theme(legend.position = "bottom")# 将图例移动到左侧
  


# 繪製特征相關性，前幾个最重要的变量
var_imp <- summary(gbm_model, plot=FALSE)
top_vars <- head(var_imp, 4)$var

# 创建一个空的图形列表
plots_cor <- list()

# 为前6个变量创建偏差图
for (i in seq_along(top_vars)) {
  plots_cor[[i]] <- plot.gbm(gbm_model, i = top_vars[i])
}

# 使用grid.arrange来组合图形
# combined_plots_cor = do.call(gridExtra::grid.arrange, c(plots_cor, ncol=2))
combined_plots_cor <- gridExtra::arrangeGrob(grobs = plots_cor, ncol = 2)

# 使用cowplot将两个图形组合起来
combined_plot <- plot_grid(importance_plot,
                           ggplotify::as.ggplot(combined_plots_cor),
                           labels = c("Feature Importance","partial dependence"),
                           ncol = 2,
                           rel_heights = c(1, 1),
                           rel_widths = c(1,1))
combined_plot


# # 残差
# residuals <- abs(gbm_test$MI_all_outcome - gbm_test$predict)
# gbm_rmes = data.frame(residuals = residuals)
# 
# # 繪製残差圖
# ggplot(gbm_rmes, aes(x = residuals)) +
#   geom_histogram(binwidth = 0.5, fill = "darkslategray", color = "black", alpha = 0.7) +
#   ggtitle("Residuals Distribution")
# 
# # learning line
# train_error <- gbm.perf(gbm_model, method = "cv")

```

Because MI_all_outcome is a categorical variable, i.e., its values are
only 0 and 1. Then when modeling with GBM (Gradient Boosting Machine),
we should use the binomial distribution ("bernoulli") method.

We used **Youden's J statistic** to choose threshold for prediction, This
is a statistical method used to select a threshold that maximizes the
difference between the true and false positive rates to find a good
balance.

Feature Importance Bar Chart : In this graph, '**cholesterol_0**'
(cholesterol) is the most important feature, followed by '**age**' (age) ,
'**bmi_0**' (BMI) and '**sex**'(gender). This means that these variables
have the most influence in the model predictions, especially
**cholesterol** that occupy 50%.

partial dependence plots : **cholesterol** : The graph shows a rapid
decline in the predicted value of Y as the cholesterol level increases
from the lowest value to about 4 units, and then a stabilization of the
predicted value of Y as the cholesterol level increases further. This
may mean that cholesterol levels have a strong negative effect on Y
within a certain range, but once this range is exceeded, the effect
becomes less pronounced.

**age** : As age increases, the predicted value of Y shows a gradual
increase, especially at around 60 years of age, where the predicted
value increases more significantly. This suggests that age may be a
factor in the increase in Y, especially at older ages.

**bmi_0** : For BMI, the predicted value of Y has a spike at a BMI of
about 30 units and then stabilizes. This may indicate that there is a
significant association between BMI and Y in the middle range of the
model, but that this association no longer strengthens once BMI reaches
a certain point.

**sex** : For gender, we see two different levels, indicating that males
and females have significantly different predicted impacts on Y in the
model. We can say that male(1) has more chance to get myocardial
infarction.

Finally, this model has approximate 70 % Accuracy.

### Dementia

```{r, fig.cap="Figure 9 : Dementia model interpretatIon"}

gbm_model <- gbm(dementia_all_outcome ~ .,
                 data = gbm_train,
                 distribution = "bernoulli", 
                 n.trees = 300,               
                 shrinkage = 0.01,            
                 interaction.depth = 3,     
                 cv.folds = 5)               

predicted <- predict(gbm_model, newdata = gbm_test, type = "response")

# 计算ROC曲线
roc_obj <- roc(gbm_test$dementia_all_outcome, predicted)

# 计算Youden's J statistic
youdens_j <- roc_obj$sensitivities + roc_obj$specificities - 1

# 找到最大的Youden's index对应的阈值
optimal_cutoff <- roc_obj$thresholds[which.max(youdens_j)]
cat("Threshold corresponding to the largest Youden's index : ",optimal_cutoff)
# 根据阈值决定类别，通常使用0.5作为阈值
predicted_class <- ifelse(predicted > optimal_cutoff, 1, 0)

# 将预测类别添加到测试数据集中
gbm_test$predict <- predicted_class

# 计算准确率
accuracy <- mean(gbm_test$predict == gbm_test$dementia_all_outcome)

cat("\n","Accuracy: ", round(accuracy * 100, 2), "%")

# 繪製特征重要性
importance <- summary(gbm_model, n.trees = 100, plot = FALSE)
df_importance <- data.frame(feature = rownames(importance), relative_importance = importance$rel.inf)
importance_plot = ggplot(df_importance, aes(x = reorder(feature, -relative_importance), y = relative_importance,  fill = relative_importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Features") + ylab("Relative Importance") +
  scale_fill_gradient(low = "darkslategray3", high = "darkslategray") +
  # ggtitle("Feature Importance")+
  theme(legend.position = "bottom")# 将图例移动到左侧
  


# 繪製特征相關性，前幾个最重要的变量
var_imp <- summary(gbm_model, plot=FALSE)
top_vars <- head(var_imp, 4)$var

# 创建一个空的图形列表
plots_cor <- list()

# 为前6个变量创建偏差图
for (i in seq_along(top_vars)) {
  plots_cor[[i]] <- plot.gbm(gbm_model, i = top_vars[i])
}

# 使用grid.arrange来组合图形
# combined_plots_cor = do.call(gridExtra::grid.arrange, c(plots_cor, ncol=2))
combined_plots_cor <- gridExtra::arrangeGrob(grobs = plots_cor, ncol = 2)

# 使用cowplot将两个图形组合起来
combined_plot <- plot_grid(importance_plot,
                           ggplotify::as.ggplot(combined_plots_cor),
                           labels = c("Feature Importance","partial dependence"),
                           ncol = 2,
                           rel_heights = c(1, 1),
                           rel_widths = c(1,1))
combined_plot


# # 残差
# residuals <- abs(gbm_test$dementia_all_outcome - gbm_test$predict)
# gbm_rmes = data.frame(residuals = residuals)
# 
# # 繪製残差圖
# ggplot(gbm_rmes, aes(x = residuals)) +
#   geom_histogram(binwidth = 0.5, fill = "darkslategray", color = "black", alpha = 0.7) +
#   ggtitle("Residuals Distribution")
# 
# # learning line
# train_error <- gbm.perf(gbm_model, method = "cv")

```

Same as MI_all_outcome, dementia_all_outcome is a categorical variable.
we use the binomial distribution ("bernoulli") method for modeling.

We used **Youden's J statistic** to choose threshold for prediction, This
is a statistical method used to select a threshold that maximizes the
difference between the true and false positive rates to find a good
balance.

Feature Importance Bar Chart : In this graph, '**age**' (age) is the most
important feature, followed by , '**bmi_0**' (BMI), '**cholesterol_0**'
(cholesterol) and '**townsend_deprivation_index**'(deprivation index).
This means that these variables have the most influence in the model
predictions, especially **age**.

partial dependence plots : **age** : As age increases, the predicted value
of Y shows a gradual increase, especially at around 60 years of age,
where the predicted value increases more significantly. This suggests
that age may be a factor in the increase in Y, especially at older ages.

**bmi_0** : It shows an interesting non-linear relationship where the
predicted value of Y decreases in certain BMI ranges, especially in the
range of about 30. This means that in this range, the effect of
increasing BMI on Y is negative. In other ranges, however, the
relationship appears to become positive or insignificant.

**cholesterol_0** : In the graph of cholesterol levels, we see several
sharp peaks(3.5 , 8) and valleys(6 to 8) that show significant changes
in the predicted value of Y at specific cholesterol levels. This
indicates that the model is very sensitive to changes in cholesterol
levels, especially at certain specific points, which may mean that there
is a specific effect of cholesterol levels on the predicted variables
around these points

**townsend_deprivation_index** : For the Townsend Deprivation Index, the
graph shows a complex nonlinear relationship. The predicted value of Y
appears to be lowest when the index is near 0, while the predicted value
increases at higher or lower values of the deprivation index. This may
indicate that deprivation has a smaller effect on Y in the medium range
and a larger effect at higher levels of deprivation.

Finally, this model has approximate 70 % Accuracy.

### Stroke

```{r, fig.cap="Figure 10 : Stroke model interpretatIon"}

gbm_model <- gbm(stroke_all_outcome ~ .,
                 data = gbm_train,
                 distribution = "bernoulli", 
                 n.trees = 300,               
                 shrinkage = 0.01,            
                 interaction.depth = 3,     
                 cv.folds = 5)               

predicted <- predict(gbm_model, newdata = gbm_test, type = "response")

# 计算ROC曲线
roc_obj <- roc(gbm_test$stroke_all_outcome, predicted)

# 计算Youden's J statistic
youdens_j <- roc_obj$sensitivities + roc_obj$specificities - 1

# 找到最大的Youden's index对应的阈值
optimal_cutoff <- roc_obj$thresholds[which.max(youdens_j)]
cat("Threshold corresponding to the largest Youden's index : ",optimal_cutoff)
# 根据阈值决定类别，通常使用0.5作为阈值
predicted_class <- ifelse(predicted > optimal_cutoff, 1, 0)

# 将预测类别添加到测试数据集中
gbm_test$predict <- predicted_class

# 计算准确率
accuracy <- mean(gbm_test$predict == gbm_test$stroke_all_outcome)

cat("\n","Accuracy: ", round(accuracy * 100, 2), "%")

# 繪製特征重要性
importance <- summary(gbm_model, n.trees = 100, plot = FALSE)
df_importance <- data.frame(feature = rownames(importance), relative_importance = importance$rel.inf)
importance_plot = ggplot(df_importance, aes(x = reorder(feature, -relative_importance), y = relative_importance,  fill = relative_importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Features") + ylab("Relative Importance") +
  scale_fill_gradient(low = "darkslategray3", high = "darkslategray") +
  # ggtitle("Feature Importance")+
  theme(legend.position = "bottom")# 将图例移动到左侧
  


# 繪製特征相關性，前幾个最重要的变量
var_imp <- summary(gbm_model, plot=FALSE)
top_vars <- head(var_imp, 4)$var

# 创建一个空的图形列表
plots_cor <- list()

# 为前6个变量创建偏差图
for (i in seq_along(top_vars)) {
  plots_cor[[i]] <- plot.gbm(gbm_model, i = top_vars[i])
}

# 使用grid.arrange来组合图形
# combined_plots_cor = do.call(gridExtra::grid.arrange, c(plots_cor, ncol=2))
combined_plots_cor <- gridExtra::arrangeGrob(grobs = plots_cor, ncol = 2)

# 使用cowplot将两个图形组合起来
combined_plot <- plot_grid(importance_plot,
                           ggplotify::as.ggplot(combined_plots_cor),
                           labels = c("Feature Importance","partial dependence"),
                           ncol = 2,
                           rel_heights = c(1, 1),
                           rel_widths = c(1,1))
combined_plot

# # 残差
# residuals <- abs(gbm_test$stroke_all_outcome - gbm_test$predict)
# gbm_rmes = data.frame(residuals = residuals)
# 
# # 繪製残差圖
# ggplot(gbm_rmes, aes(x = residuals)) +
#   geom_histogram(binwidth = 0.5, fill = "darkslategray", color = "black", alpha = 0.7) +
#   ggtitle("Residuals Distribution")

# learning line
# train_error <- gbm.perf(gbm_model, method = "cv")

```

Same as MI_all_outcome, stroke_all_outcome is a categorical variable. we
use the binomial distribution ("bernoulli") method for modeling.

We used **Youden's J statistic** to choose threshold for prediction, This
is a statistical method used to select a threshold that maximizes the
difference between the true and false positive rates to find a good
balance.

Feature Importance Bar Chart : In this graph, '**age**' (age) is the most
important feature, followed by , '**cholesterol_0**' (cholesterol),
'**dementia_all_outcomeand**'(dementia) and '\*MET"\_acticity\*'(measure
for exercise). This means that these variables have the most influence
in the model predictions.

partial dependence plots : **age** : This graph shows that as age
increases, the predicted value of Y shows an increasing trend,
especially in the 60-70 age range, where this trend is very pronounced.
This may mean that the effect of age on the predicted outcome increases
at that age, which may indicate a positive correlation between age and
the predicted target.

**Cholesterol_0** :In the dependency plot for cholesterol levels, it can
be observed that the predicted value of Y decreases significantly when
cholesterol levels are in the range of specific low values. However, for
most of the range of cholesterol levels, the value of Y is relatively
stable, suggesting that it only has a significant impact on the
predicted target at specific low cholesterol levels.

**Townsend_deprivation_index** : This graph shows a significant peak in
the effect of the Thomson Deprivation Index on the predicted value of Y,
which occurs when the index is close to 0. This may indicate that the
predicted value of Y is lowest at moderate levels of deprivation and
increases at higher or lower levels of deprivation. This may indicate
that the predicted value of Y is lowest at moderate levels of
deprivation and increases at higher or lower levels of deprivation.

**MET_activity** : The graph of MET activity levels shows that the
predicted value of Y is relatively stable at low activity levels, with a
slight increase in the range from 2000 to about 6000. However, at very
low activity levels or close to 10,000, the predicted value of Y
increase significantly. This may indicate that only at very high or very
low activity levels does the MET activity level have a significant
effect on the predicted results.

Finally, this model has approximate 70 % Accuracy.

# Conclusion

Based on the provided data and model results, we can summarize the effect of cholesterol (Cholesterol) in different disease prediction models as follows:

### Cholesterol Distribution: 
Appears roughly normal with slight right-skewness. No substantial difference in cholesterol levels when comparing gender. Variance is seen across ethnic groups, with Asian and White ethnicities displaying the highest maximum cholesterol levels.

### Effect of Cholesterol on Myocardial Infarction (MI) Modeling:

Cholesterol is one of the most important features in the Myocardial Infarction (MI) prediction model, especially since it accounts for 50% of the feature importance.
From the Partial Dependence Plots, it can be observed that the predictive value Y decreases rapidly as the cholesterol level increases from the lowest value to approximately 4 units, and then stabilizes as the cholesterol level increases further.
This may indicate that cholesterol levels have a strong negative effect on the risk of myocardial infarction within a certain range, but that the effect diminishes beyond this range.

### Effect of cholesterol on the cognitive disorder (Dementia) model:

In models of cognitive disorders, cholesterol is less important than age and BMI, but still has a place among the important characteristics.
The dependency plots show spikes and troughs at specific cholesterol level points (3.5 and 8 units), suggesting that the model is very sensitive to specific points in cholesterol levels, and may imply that cholesterol levels around these points have a specific effect on the predicted variables.

### Effect of cholesterol on stroke (Stroke) modeling:

Cholesterol is also an important feature in stroke prediction models.
The dependency plot shows that when cholesterol levels are within a specific range of low values, the predicted value of Y decreases significantly. However, Y was relatively stable over most of the range of cholesterol levels, suggesting that cholesterol levels have a significant effect on the predictive target only within a specific low range of values(2 to 4).

To sum up, cholesterol levels are notably influential in the prognostic models for cardiovascular conditions such as myocardial infarction and stroke, particularly within the low to moderate spectrum (**2 to 4 units**). It is imperative that clinical attention is heightened when patient cholesterol levels fall within this critical range, as it may significantly alter the predictive outcomes and thereby inform more targeted intervention strategies.
However, in models of brain dysfunction, the effect of **cholesterol** appears to be smaller than that of other characteristics such as age and socioeconomic status. Despite the differences in the predictive contribution of cholesterol to these diseases, maintaining healthy cholesterol levels remains important in the prevention and management of these diseases.

# Recommendations for Further Analysis:
Deeper Statistical Analysis: To clarify the negative correlation between cholesterol and MI outcomes. 
Expanded Data Collection: To better understand cholesterol’s role across different demographics and health profiles.

